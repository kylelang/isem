%%% Title:    IDS: Lecture 1
%%% Author:   Kyle M. Lang
%%% Created:  2018-04-06
%%% Modified: 2020-04-03

\documentclass{beamer}
\usetheme[%
  pageofpages          = of,
  bullet               = circle,
  titleline            = true,
  alternativetitlepage = true,
  titlepagelogo        = Logo3,
  watermark            = watermarkTiU,
  watermarkheight      = 100px,
  watermarkheightmult  = 4%
]{UVT}

\usepackage{graphicx}
\usepackage[natbibapa]{apacite}
\usepackage[libertine]{newtxmath}

%% Ensure styles of `blocks' (used in Definitions, Theorems etc.) follows the
%% UVT-style theme:
\setbeamercolor{block title}{fg = darkblue, bg = white}
\setbeamercolor{block body}{use = block title, bg = block title.bg}

%% Ensure TableOfContents is in UVT-style theme:
\setbeamercolor{section in toc}{fg = darkblue}

\title{Introduction}
\subtitle{Introduction to Data Science Lecture 1}
\author{Kyle M. Lang}
\institute{Department of Methodology \& Statistics\\Tilburg University}
\date{Block 4 2020}

\newcommand{\R}{\textsf{R}}

\begin{document}

<<setup, include=FALSE>>=
set.seed(235711)

library(knitr)
library(ggplot2)
library(plyr)

source("../../../code/supportFunctions.R")

opts_chunk$set(size = 'footnotesize', fig.align = 'center')
knit_theme$set('edit-kwrite')
options(width = '60')

lightBlue <- rgb(0, 137, 191, max = 255)
midBlue   <- rgb(0, 131, 183, max = 255)
darkBlue  <- rgb(0, 128, 179, max = 255)
deepGold  <- rgb(184, 138, 45, max = 255)
lightGold <- rgb(195, 146, 48, max = 255)
@

\watermarkon %-----------------------------------------------------------------%

\begin{frame}[t,plain]
\titlepage
\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}{Outline}

  \begin{enumerate}
  \item Overview of this course
    \vb
  \item What is data science?
    \vb
  \item Data science workflow
    \vb
  \item Data science novelties
    \vb
  \item Statistical modeling
  \end{enumerate}
  
\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}{}
  
  \begin{center}
    \Huge{\textsc{Course Outline}}
  \end{center}
  
\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}{Course Road-map}

  This course will be broken into two parts:
  \va
  \begin{itemize}
  \item We will spend the first three weeks on \emph{supervised learning} 
    methods.
    \vb
  \item Around the beginning of May, we'll begin discussing \emph{unsupervised
    learning} methods.  
  \end{itemize}

\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}{Course Structure}
  
  \begin{itemize}
  \item For the first three weeks:
    \vc
    \begin{itemize}
    \item Two plenary lecture sessions per week 
      \begin{itemize}
      \item These lectures will be streamed via Zoom.
      \item Hopefully, the live-stream will be recorded and posted to Canvas for 
        students with time-zone difficulties.
      \end{itemize}
      \vc
    \item Two lab meetings per week 
      \begin{itemize}
      \item I will pre-record a demonstration lecture and post the video to 
        Canvas.
      \item During the scheduled lab session, I will be available for questions 
        via Zoom.
      \end{itemize}
    \end{itemize}
    \vb
  \item For the final four weeks:
    \vc
    \begin{itemize}
    \item One plenary lecture session per week 
      \vc
    \item One lab meeting per week 
      \vc
    \item Kim will flesh-out the details at a later date.
    \end{itemize}
  \end{itemize}

\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}{Grading \& Evaluation}
  
  \begin{itemize}
   \item You will complete two group assignments.
    \begin{itemize}
    \item One on supervised learning
    \item One on unsupervised learning
    \end{itemize}
    \vb
  \item The course is rounded-off with a written final exam.
    \vb
  \item Your course grade will be a weighted average of the grades you receive
    for the group assignments and your exam grade.  
    \begin{itemize}
      \item The assignments will contribute 40\% to your grade.
      \item The exam will contributed 60\% to your grade.
      \item The three grades \emph{can} compensate one another.
    \end{itemize}
  \end{itemize}
  
\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}{}
  
  \begin{center}
    \Huge{\textsc{Defining Data Science}}
  \end{center}
  
\end{frame}

%------------------------------------------------------------------------------%
\comment{%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{What is data science?}
  
  Well, what do you think?\\
  \va
  
  \pause
  
  Is ``data science'' a dirty word?
  \begin{itemize}
  \item What connotations do you have for the term ``data science?''
  \end{itemize}
  
\end{frame}
}%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%------------------------------------------------------------------------------%

\begin{frame}{What is data science?}
  
  At the very least, ``data science'' is a buzzword.
  \begin{itemize}
  \item It's also a job
    \begin{itemize}
    \item You can certainly be hired as a data scientist.
    \end{itemize}
  \end{itemize}
  \va
  
  \pause
  
  In a strict sense, ``data science'' is almost certainly a misnomer.
  \begin{itemize}
  \item Data science is not the \emph{science} of data.
  \end{itemize}
  
\end{frame}

\watermarkoff %----------------------------------------------------------------%

\begin{frame}{What is data science?}
  
  A mixture of skills and a merger of disciplines:
   \begin{columns}
    \begin{column}{0.4\textwidth}
  
      \begin{itemize}
      \item Statistics
      \item Computer science
      \item Mathematics
      \item Programming
      \item Data processing
      \item Data visualization
      \item Communication
      \item Substantive expertise
      \end{itemize}
      
    \end{column}
    \begin{column}{0.5\textwidth}
      
      \begin{figure}
        \includegraphics[width = 1.1\textwidth]{figures/data_science_pyramid.png}
      \end{figure}
     
    \end{column}
   \end{columns}
   \va
   \tiny{Figure source: 
     \url{https://hackernoon.com/the-ai-hierarchy-of-needs-18f111fcc007}}
     
\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}{Aside: Confusing Nomenclature}
  
  When you start studying this stuff, you will encounter a dizzying array of
  terms that all seem to describe the same things. Here's why:
  
  \begin{figure}
    \includegraphics[width = 0.65\textwidth]{figures/field_tree2.pdf}
  \end{figure}
  
\end{frame}

\watermarkon %-----------------------------------------------------------------%

\begin{frame}{What is data science?}
  
  A focus on practical problem solving
  \begin{itemize}
  \item Data analysis should create value.
    \begin{itemize} 
    \item We're trying to extract knowledge from data.
      %\end{itemize}
    \item Start with a question and use data to answer it.
      %\begin{itemize}
    \item Don't start with data and generate answerable questions.
    \end{itemize}
  \item Use appropriately complex methods.
    \begin{itemize}
    \item Don't waste resources on complex analyses when simpler analyses will
      solve your problem equally well.
    \item Don't settle for bad answers just because good answers will require
      complex/difficult analyses.
    \end{itemize}
  \item Don't ask if you \emph{can}; ask if you \emph{should}.
    \begin{itemize}
    \item Why are you doing a particular analysis?
    \item All analytic decisions should be justified.
    \end{itemize}
  \end{itemize}
  
\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}{What is data science?}
  
  A strong focus on pragmatism and skepticism
  \begin{itemize}
  \item Don't be tied to a ``pet method''.
  \item Embrace exploratory methods.
    \begin{itemize}
    \item Don't overgeneralize exploratory findings.
    \end{itemize}
  \item Treat neither data nor theory as sacred.
    \begin{itemize}
    \item Don't sanctify theory in the face of (definitively) contradictory data.
    \item Don't blithely let data overrule well-supported theory.
    \end{itemize}
  \item Trust no one.
    \begin{itemize}
    \item Not data, other people, or yourself
    \item Check and double check
    \end{itemize}
  \item Don't assume what can be tested.
  \item When in doubt, err on the side of conservative inference.
  
  \item Document everything!
  \end{itemize}
  
\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}{What is data science?}
  
  A fast-paced, curious, open-minded attitude
  \begin{itemize}
  \item Iterate quickly, fail quickly
  \item Never stop learning.
    \begin{itemize}
    \item Learn and use new methods
    \item Always remain open to new ideas/approaches.
    \end{itemize}
  \item Don't be afraid to tackle new problems.
    \begin{itemize}
    \item Generalize and extend what you know.
    \item Don't stagnate.
    \end{itemize}
  \item Show an appropriate degree of humility.
    \begin{itemize}
    \item You don't know everything.
      \begin{itemize}
      \item Embrace and correct your ignorance.
      \end{itemize}
    \item Ask questions.
      \begin{itemize}
      \item Communicate. Don't just talk.
      \end{itemize}
    \end{itemize}
  \end{itemize}

\end{frame}

%------------------------------------------------------------------------------%
\comment{%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{What is data science?}
  
  \begin{columns}
    \begin{column}{0.5\textwidth}
  
      
    \end{column}
    \begin{column}{0.5\textwidth}
      
      \begin{figure}
        \includegraphics[width = 0.9\textwidth]{figures/data_science_pyramid.png}
      \end{figure}
      
    \end{column}
  \end{columns}
  
\end{frame}
}%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%------------------------------------------------------------------------------%


\begin{frame}{}
  
  \begin{center}
    \Huge{\textsc{Data Science Workflow}}
  \end{center}
  
\end{frame}

\watermarkoff %----------------------------------------------------------------%

\begin{frame}{Research Cycle}

  The following is a representation of the \emph{Research Cycle} used for 
  empirical research in most of the sciences.
  
  \begin{figure}
    \includegraphics[width = 0.7\textwidth]{figures/research_cycle.pdf}
  \end{figure}
  
\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}{Data Science Cycle}
  
  The \emph{Data Science Cycle} represented here was adapted from 
  \citet{o'neilSchutt:2014}.
  
  \begin{figure}
    \includegraphics[width = 0.7\textwidth]{figures/data_science_cycle.pdf}
  \end{figure}
  
\end{frame}

\watermarkon %-----------------------------------------------------------------%

\begin{frame}{}
  
  \begin{center}
    \Huge{\textsc{Data Science Novelties}}
  \end{center}
  
\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}{Novel Data Structures}
  
  In the social and behavioral sciences, we are accustomed to analyzing small,
  rectangular datasets.
  \begin{itemize}
  \item Rows represent observational units.
  \item Columns represent variables.
  \end{itemize}
  \vb
  Data science applications deal with much more diverse forms of data.
  \begin{columns}
    \begin{column}{0.4\textwidth}
      
      \begin{itemize}
      \item Relational databases
      \item Data streams
      \item Web logs
      \end{itemize}
  
    \end{column}
    \begin{column}{0.4\textwidth}
      
      \begin{itemize}
      \item Sensor data
      \item Image data
      \item Unstructured text
      \end{itemize}
      
    \end{column}
  \end{columns}
  \va
  These datasets are often much larger and less structured that those 
  traditionally analyzed in the social and behavioral sciences.
  
\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}{Parallel Processing/Distributed Computing}
  
  When dealing with large amounts of (distributed) data, we should move the data
  as little as possible.
  \begin{itemize}
  \item We can analyze distributed data \emph{in situ} without moving them to a 
    central computer.
    \begin{itemize}
    \item Distributed computing
    \end{itemize}
  \end{itemize}
  \vb
  When executing long-running jobs, we should try to split the calculations into
  smaller pieces that can be executed simultaneously.
  \begin{itemize}
  \item Parallel processing
  \end{itemize}
  \vb
  Parallel processing comes in two flavors:
  \begin{itemize}
  \item Embarrassingly Parallel
  \item Multi-threading
  \end{itemize}
  
\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}{Parallel Processing Technologies}
  
  We can distribute embarrassingly parallel jobs directly.
  \begin{itemize}
  \item No real need for clever task partitioning
  \end{itemize}
  \vb
  We must break multi-threaded jobs into independent subtasks.
  \begin{itemize}
  \item Several technologies can facilitate multi-threading.
    \begin{itemize}
    \item Small-scale:
      \begin{itemize}
      \item Message Passing Interface (MPI)
      \item Open Multi Processing (OpenMP)
      \end{itemize}
    \item Large-scale:
      \begin{itemize}
      \item Google's MapReduce algorithm
      \item Apache Hadoop
      \item Apache Spark
      \end{itemize}
    \end{itemize}
  \end{itemize}
  \vb
  Some software can use parallel processing behind-the-scenes.
  
\end{frame}

\watermarkoff %----------------------------------------------------------------%

\begin{frame}[fragile]{Parallel Processing Example}
  
  \begin{columns}
    \begin{column}{0.5\textwidth}
      
      Run a Monte Carlo simulation to test the central limit theorem.
      \begin{itemize}
      \item Population model:
        \begin{itemize}
        \item $x_p \sim \Gamma(0.5, 1.0)$
        \end{itemize}
      \item Parameters:
        \begin{itemize}
        \item $P \in \{1, 2, \ldots, 50\}$
        \item $N \in \{5, 10, \ldots, 100\}$
        \end{itemize}
      \item Mean score for the $n$th row:
        \begin{itemize}
        \item $\bar{x}_n = P^{-1}\sum_{p = 1}^P x_{np}$
        \end{itemize}
      \item Outcome:
        \begin{itemize}
        \item KS statistic testing if $\bar{x}_n$ is normally distributed
        \end{itemize}
      \end{itemize}
      
    \end{column}
    \begin{column}{0.5\textwidth}

<<echo = FALSE>>=
x <- seq(0, 5, length.out = 1000)
gg0(x = x, y = dgamma(x, 0.5, 1), points = FALSE) + 
    geom_line() +
    xlab("X") +
    ylab("Density")
@ 

    \end{column}
  \end{columns}
  
\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}[fragile]{Parallel Processing Example}

  First, we'll define a function to run one replication of the simulation:
  
<<>>=
## Run one replication of the simulation:
doRep <- function(rp, conds) {
    res <- rep(NA, nrow(conds))
    for(i in 1 : nrow(conds)) { # Loop through conditions
        ## Compute a mean score from p variables:
        x <- rowMeans(
            replicate(conds[i, "p"], 
                      rgamma(conds[i, "n"], 0.5, 1)
                      )
        )
        
        ## Calculate the KS statistic:
        res[i] <- ks.test(x, "pnorm", mean(x), sd(x))$stat
    }
    cbind(conds, res) # Return the results
}
@ 

\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}[fragile]{Parallel Processing Example}

  Then, we prepare the environment:
  
<<>>=
library(parallel) # We'll need this for parallel processing
library(lattice)  # We'll use this for plotting

## Define simulation conditions:
nVec  <- seq(5, 100, 5)
pVec  <- 1 : 50
conds <- expand.grid(n = nVec, p = pVec)

## How many replications?
nReps <- 500
@ 

\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}[fragile]{Parallel Processing Example}

We'll run the simulation in two ways:

<<cache = TRUE>>=
# Run 'nReps' replications in a loop:
t1 <- system.time(
{
    out1 <- list()
    for(rp in 1 : nReps)
        out1[[rp]] <- doRep(rp, conds = conds)
}
)

## Run 'nReps' replications in parallel using mclapply():
t2 <- system.time(
    out2 <- mclapply(X        = 1 : nReps,
                     FUN      = doRep,
                     conds    = conds,
                     mc.cores = 2)
)
@ 

\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}[fragile]{Parallel Processing Example}
  
  We can visualize the results of our simulation:
  
  \begin{columns}
    \begin{column}{0.5\textwidth}
      
<<echo = FALSE>>=
tmp  <- do.call(rbind, out1)
agg  <- aggregate(tmp[ , 3], by = tmp[ , 1 : 2], mean)
res1 <- data.frame(N = conds[ , "n"], P = conds[ , "p"], KS = agg[ , 3])

wireframe(x           = KS ~ N * P,
          data        = res1,
          drape       = TRUE,
          col.regions = colorRampPalette(c("blue", "red"))(100),
          screen      = list(z = -55, x = -80),
          main        = "Serial Results"
          )
@
      
    \end{column}
    \begin{column}{0.5\textwidth}

<<echo = FALSE>>=
tmp  <- do.call(rbind, out2)
agg  <- aggregate(tmp[ , 3], by = tmp[ , 1 : 2], mean)
res2 <- data.frame(N = conds[ , "n"], P = conds[ , "p"], KS = agg[ , 3])

wireframe(x           = KS ~ N * P,
          data        = res2,
          drape       = TRUE,
          col.regions = colorRampPalette(c("blue", "red"))(100),
          screen      = list(z = -55, x = -80),
          main        = "Parallel Results"
          )
@ 

    \end{column}
  \end{columns}
  
\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}[fragile]{Parallel Processing Example}
  
  Finally, we'll compare the computational speed of the parallel and serial
  approaches:
  
<<>>=
## Serial version:
t1

## Parallel version:
t2
@ 

Running the program in parallel substantially speeds computation.
\begin{itemize} 
\item The parallel version is \Sexpr{round(t1["elapsed"] / t2["elapsed"], 2)} 
  times faster than the serial version.  
\end{itemize}

\end{frame}

\watermarkon %-----------------------------------------------------------------%

\begin{frame}{Computationally Efficient Algorithms}
  
  In general, there will be many ways to estimate a given model.
  \begin{itemize}
  \item When dealing with large data structures, choosing a computationally
    efficient approach is important.
  \end{itemize}
  \vb
  There will usually be a trade-off between memory efficiency and computational
  efficiency.
  \begin{itemize}
  \item We can compute faster by storing the result of initial calculations, but
    doing so entails higher memory usage.
  \end{itemize}
  \vb 
  Certain data structures should be analyzed with specialized computational
  techniques.
  \begin{itemize}
  \item Data streams $\rightarrow$ online learning, batch processing
  \item Distributed data $\rightarrow$ distributed computing
  \end{itemize}
  
\end{frame}

\watermarkoff %----------------------------------------------------------------%

\begin{frame}[fragile]{Online Learning Example}
  
  \begin{columns}
    \begin{column}{0.35\textwidth}
      
      Suppose we want to estimate the mean of $X$.
      \begin{itemize}
      \item Traditional approach:
      \end{itemize}
      \vx{-6}
      \begin{align*}
        \bar{X} = N^{-1}\sum_{n = 1}^N x_n
      \end{align*}
      Maybe, we don't want to keep all of $X$ in memory.
      \begin{itemize}
      \item Online updating:
      \end{itemize}
      \vx{-6}
      \begin{align*}
        \bar{X}_{n} = \frac{(n - 1)\bar{X}_{n - 1} + x_n}{n}
      \end{align*}
      
    \end{column}
    \begin{column}{0.6\textwidth}
      
<<>>=
## Generate some data:
x <- runif(10000)

## Traditional estimation:
m0 <- sum(x) / length(x)

## Online updating:
m1 <- x[1]
for(n in 2 : length(x))
    m1 <- ((n - 1) * m1 + x[n]) / n

## Compare results:
m0; m1
@ 

    \end{column}
  \end{columns}
  
\end{frame}

\watermarkon %-----------------------------------------------------------------%
    
\begin{frame}{}
  
  \begin{center}
    \Huge{\textsc{Statistical Modeling}}
  \end{center}
  
\end{frame}

%------------------------------------------------------------------------------%
\comment{%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{Probability Distributions}

  Before going any further, we'll review the general concept of a probability
  distribution.
  \va
  \begin{itemize}
  \item Probability distributions quantify how likely is each possible value of
    some probabilistic entity.
    \va
  \item Probability distributions are re-scaled frequency distributions.
    \va
  \item We can build up the intuition of a probability density by beginning with
    a histogram.
  \end{itemize}

\end{frame}

\watermarkoff %----------------------------------------------------------------%

\begin{frame}{Probability Distributions}

<<echo = FALSE, cache = TRUE>>=
n      <- 1e5
myBins <- c(10, 25, 50, 100)

dat1 <- data.frame(x = rep(rnorm(n), length(myBins)),
                   y = rep(myBins, each = n)
                   )
@

\begin{columns}
  \begin{column}{0.5\textwidth}

<<echo = FALSE, cache = TRUE, out.width = '\\linewidth'>>=
p1 <- ggplot(dat1, aes(x = x)) + theme_classic() +
    coord_cartesian(xlim = c(-4, 4))

geomList <- mapply(function(x, b) geom_histogram(data = x,
                                                 bins = b,
                                                 col = "white",
                                                 fill = midBlue),
                   dlply(dat1, .(y)),
                   b = myBins
                   )

labs <- c("10"  = "10 Bins",
          "25"  = "25 Bins",
          "50"  = "50 Bins",
          "100" = "100 Bins")

p1 + geomList +
    facet_wrap(~y, scales = "free_y", labeller = as_labeller(labs)) +
    theme(strip.background = element_blank(),
          strip.placement = "outside",
          text = element_text(size = 16, family = "Courier"))
@

  \end{column}

  \begin{column}{0.5\textwidth}

<<echo = FALSE, cache = TRUE, out.width = '\\linewidth'>>=
p2 <- ggplot(dat1, aes(x = x, y = ..density..)) + theme_classic() +
    coord_cartesian(xlim = c(-4, 4))

geomList <- mapply(function(x, b) geom_histogram(data = x,
                                                 bins = b,
                                                 col = "white",
                                                 fill = midBlue),
                   dlply(dat1, .(y)),
                   b = myBins
                   )

labs <- c("10"  = "10 Bins",
          "25"  = "25 Bins",
          "50"  = "50 Bins",
          "100" = "100 Bins")

p2 + geomList +
    facet_wrap(~y, scales = "free_y", labeller = as_labeller(labs)) +
    theme(strip.background = element_blank(),
          strip.placement = "outside",
          text = element_text(size = 16, family = "Courier"))
@

\end{column}
\end{columns}

\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}{Probability Distributions}

  \begin{columns}
    \begin{column}{0.5\textwidth}
      In the hypothetical limit as $N \rightarrow \infty$, a histogram smooths
      into a continuous curve. 
      \va
      \begin{itemize}
      \item Each point on the curve gives the probability of observing the
        corresponding $X$ value in the population.
        \va
      \item The area under the curve must integrate to 1.0.
      \end{itemize}
    \end{column}

    \begin{column}{0.5\textwidth}

<<echo = FALSE, cache = TRUE>>=

x <- seq(-4.0, 4.0, 0.001)

dat2 <- data.frame(X = x, density = dnorm(x))

p3 <- ggplot(dat2, aes(x = X, y = density)) + theme_classic() +
    coord_cartesian(xlim = c(-4, 4))

p4 <- p3 + geom_area(fill = midBlue) +
    theme(text = element_text(size = 16, family = "Courier"))

p4
@

\end{column}
\end{columns}

\end{frame}

\watermarkon %-----------------------------------------------------------------%

\begin{frame}[allowframebreaks]{Motivating Example}

  You work for a tech marketing group, and you have been tasked with
  developing recommendations for your company's customers.
  \va
  \begin{itemize}
  \item Two new mobile phones will soon be released.
    \va
  \item For each phone, you have access to the ratings of 100 beta testers.
    \va
  \item How to you distill those 200 scores into succinct recommendations?
  \end{itemize}

  \pagebreak

  Suppose I tell you that the mean rating for Phone A is 7.0 and the mean rating
  for Phone B is 5.0.
  \va
  \begin{itemize}
  \item Can you confidently recommend Phone A?
    \va
  \item What caveats might you consider?
  \end{itemize}

\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}{Motivating Example}

  Suppose I tell you that the standard deviation for the ratings of Phone A is
  1.5 and the standard deviation for the ratings of Phone B is 1.0.
  \va
  \begin{itemize}
  \item How would you incorporate this new information into your
    recommendations?
  \end{itemize}
  \va
  \pause
  Suppose, instead, that the standard deviation of ratings for Phone A is 15.0
  and the standard deviation for Phone B is 10.0.
  \va
  \begin{itemize}
  \item How should you adjust your appraisal of the phones' relative quality?
  \end{itemize}

\end{frame}

\watermarkoff %----------------------------------------------------------------%

\begin{frame}{Reasoning with Distributions}

  We will gain insight by conceptualizing our example problem in terms
  of the underlying distributions of ratings.

  \begin{columns}
    \begin{column}{0.5\textwidth}

<<echo = FALSE, cache = TRUE>>=
x    <- seq(0, 13, 0.001)
dat4 <- data.frame(x  = x,
                   yA = dnorm(x, 7, 1.5),
                   yB = dnorm(x, 5, 1.0)
                   )

p5 <- ggplot(data = dat4) + coord_cartesian(xlim = c(0, 13)) + theme_classic()
p6 <- p5 + geom_area(mapping = aes(x = x, y = yA),
                     fill = "red")
p7 <- p6 + geom_area(mapping = aes(x = x, y = yB),
                     alpha = 0.80,
                     fill = "blue")

p7 + labs(title = "Instance 1 Visualized", y = "density", x = "Rating") +
     theme(text = element_text(size = 16, family = "Courier"),
           plot.title = element_text(size = 20, face = "bold", hjust = 0.5)
           )
@

\end{column}

\begin{column}{0.5\textwidth}

<<echo = FALSE, cache = TRUE>>=
x    <- seq(-50, 60, 0.1)
dat5 <- data.frame(x  = x,
                   yA = dnorm(x, 7, 15.0),
                   yB = dnorm(x, 5, 10.0)
                   )

p8 <- ggplot(data = dat5) + coord_cartesian(xlim = c(-50, 60)) + theme_classic()
p9 <- p8 + geom_area(mapping = aes(x = x, y = yA),
                     fill = "red")
p10 <- p9 + geom_area(mapping = aes(x = x, y = yB),
                      alpha = 0.80,
                      fill = "blue")

p10 + labs(title = "Instance 2 Visualized", y = "density", x = "Rating") +
    theme(text = element_text(size = 16, family = "Courier"),
          plot.title = element_text(size = 20, face = "bold", hjust = 0.5)
          )
@

\end{column}
\end{columns}

\end{frame}

\watermarkon %-----------------------------------------------------------------%

\begin{frame}{Quantifying Uncertainty}

  Statisticians (and anyone who uses statistics) quantify uncertainty using
  probability distributions.
  \va
  \begin{itemize}
  \item As you continue to work closely with statistics, you will probably
    begin viewing the world in terms of distributions.
    \va
  \item Nothing is certain outside of pure mathematics.
    \va
  \item All real-world problems involve weighing distributions of possibilities.
  \end{itemize}

\end{frame}
}%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%------------------------------------------------------------------------------%

\begin{frame}[allowframebreaks]{Statistical Reasoning}

  Statistics and data science are used to answer questions about hypothetical
  populations.
  \begin{itemize}
  \item Do men have higher job satisfaction than women?
  \item Can I predict your voting behavior?
  \item Can I detect groups of people who share similar attitudes towards
    climate change?
  \end{itemize}
  \vb
  To answer these questions, we need to use \emph{statistical reasoning}.
  \begin{itemize}
  \item The foundation of all good statistical analyses is a deliberate,
    careful, and thorough consideration of uncertainty.
  \end{itemize}
  
  \pagebreak
  
  If I measure a mean satisfaction rating for men of 5.6 and a mean satisfaction
  rating for women of 5.1, does that imply higher job satisfaction for men?
  \begin{itemize}
  \item Maybe...
  \item If the satisfaction ratings are highly variable, with respect to the
    size of the mean difference, we may not care much about the observed mean
    difference.
  \item The \emph{observed} mean difference may not represent a \emph{true} mean
    difference in the population.
  \end{itemize}
  \vb
  The purpose of statistics is to systematize the way that we account for 
  uncertainty when making data-based decisions.
  
  
\end{frame}

%------------------------------------------------------------------------------%
  
\begin{frame}{Statistical Modeling}

  To implement this ``statistical reasoning,'' we could use two different 
  approaches: \emph{statistical testing} or \emph{statistical modeling}.  
  \begin{itemize}
  \item In experimental contexts, real-world ``messiness'' is controlled through
    random assignment, and statistical testing is a sufficient method of
    knowledge generation.
    \vc
  \item Apart from A/B testing, data scientists rarely have the luxury of being 
    able to conduct experiments.
    \vc
  \item Data scientists work with messy observational data and often don't have 
    questions that lend themselves to straight-forward testing.
  \end{itemize}
  \vb
  Data scientists need \emph{statistical modeling}.

\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}{Statistical Modeling}
  
  Modelers attempt to build a mathematical representation of the (interesting 
  aspects) of a data distribution.
  \begin{itemize}
  \item The model succinctly describes whatever system is being analyzed.
    \vc
  \item Beginning with a model ensures that we are learning the important 
    features of a distribution.
    \vc
  \item The modeling approach is especially important in messy data science 
    applications.
  \end{itemize}
  
\end{frame}
\comment{%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\watermarkoff %----------------------------------------------------------------%

\begin{frame}{Statistical Modeling}

  \begin{columns}
    \begin{column}{0.5\textwidth}

      To apply a modeling approach to our example problem we consider
      the combined distribution of phone ratings.
      \va
      \begin{itemize}
      \item The model we construct will explain variations in phone
        rating based on interesting features.
        \va
      \item In this simple case, the only feature we consider is
        the type of phone.
      \end{itemize}

    \end{column}

    \begin{column}{0.5\textwidth}

<<echo = FALSE, cache = TRUE>>=
rA <- scale(rnorm(100)) * 1.5 + 7.0
rB <- scale(rnorm(100)) + 5.0

dat6 <- data.frame(rating = c(rA, rB), phone = rep(c("A", "B"), each = 100))

p14 <- ggplot(data = dat6, mapping = aes(x = rating)) +
    theme_classic() +
    theme(text = element_text(size = 16, family = "Courier"))

p14 + geom_density() + xlim(c(0, 13))
@

\end{column}
\end{columns}

\end{frame}

\watermarkon %-----------------------------------------------------------------%

\begin{frame}{Modeling our Example}

  To get the same answer as our statistical test, we model the
  distribution of ratings using a single grouping factor.
  \begin{align*}
    Y_{rating} = \alpha + \beta X_{phone} + \varepsilon
  \end{align*}
  After we fit this model to a sample, the parameters $\alpha$ and
  $\beta$ are replaced by estimated statistics.
  \begin{align*}
    \hat{Y}_{rating} &= \hat{\alpha} + \hat{\beta} X_{phone}\\
    &= 5.0 + 2.0 X_{phone}
  \end{align*}

\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}[allowframebreaks, fragile]{Modeling our Example}

  We can easily fit this model in \R:

<<echo = FALSE>>=
exData <- dat6
exData$phone <- relevel(exData$phone, ref = "B")
@

<<>>=
lmOut <- lm(rating ~ phone, data = exData)
@

\end{frame}

\watermarkoff %----------------------------------------------------------------%

\begin{frame}[fragile, shrink = 5]{Modeling our Example}

<<>>=
summary(lmOut)
@

\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}[shrink = 20]{Modeling our Example}

  The model-implied outcome, $\hat{Y}_{rating}$, represents the outcome
  reproduced by our fitted model.
  \begin{itemize}
  \item The model \emph{residuals},
    $\varepsilon = \hat{Y}_{rating} - Y_{rating}$, represent the noise
    in $Y_{rating}$ after we account for $X_{phone}$.
  \end{itemize}

\begin{columns}
  \begin{column}{0.4\textwidth}

<<echo = FALSE>>=
p14 + geom_density() + xlim(c(0, 13))
@

\end{column}

\begin{column}{0.1\textwidth}

  \Huge{$\rightarrow$}

\end{column}

\begin{column}{0.4\textwidth}

<<echo = FALSE>>=
dat6$resid <- resid(lmOut)

p15 <- ggplot(data = dat6, mapping = aes(x = resid)) +
    theme_classic() +
    theme(text = element_text(size = 16, family = "Courier"))

p15 + geom_density() + xlim(c(-4, 5))
@

\end{column}
\end{columns}

Statistical modeling can be conceptualized as the process of rarefying
a data distribution until the residuals represents truly
un-interesting noise.

\end{frame}
}%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\watermarkon %-----------------------------------------------------------------%

\begin{frame}{Two Modeling Traditions}

  \citet{breiman:2001} defines two cultures of statistical modeling:
  \begin{itemize}
  \item Data models
  \item Algorithmic models
  \end{itemize}
  \vb
  \pause
  Data scientists use both types of models.
  \begin{itemize}
  \item Both types of model have strengths and weaknesses.
    \begin{itemize}
    \item Data models tend to support a priori hypothesis testing more easily.
    \item Data models also tend to provide more interpretable results.
    \item Algorithmic models can't be beat for pure power.
    \end{itemize}
    \vb
    \pause
  \item Algorithmic models are currently preferred in cutting edge
    prediction/classification applications.  
    \vb
    \pause
  \item Many models can be viewed as data models or algorithmic models,
    depending on how they're used.
  \end{itemize}
  
\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}{Characteristics of Models}

  Data models share several core features:
  \begin{itemize}
  \item Data models are built from probability distributions.
    \begin{itemize}
    \item Data models are modular.
    \end{itemize}
    \vc
  \item Data models encode our hypothesized understanding of the system we're 
    exploring.
    \begin{itemize}
    \item Data models are constructed in a ``top-down'', theory-driven way.
    \end{itemize}
  \end{itemize}

  \pause
  \vb
  
  Algorithmic models are distinct from data models in several ways:
  \begin{itemize}
  \item Algorithmic models do not have to be built from probability
    distributions.
    \begin{itemize}
    \item Often, they are based on a set of decision rules (i.e., an algorithm).
    \end{itemize}
    \vc
  \item Algorithmic models begin with an objective (i.e., a problem to solve)
    and seek the optimal solution, given the data.
    \begin{itemize}
    \item They are built in a ``bottom-up'', data-driven way.
    \end{itemize}
  \end{itemize}
  
\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}[fragile]{Data Modeling Example}

  Suppose we believe the following:
  \begin{enumerate}
  \item BMI is positively associated with disease progression in diabetic 
    patients after controlling for age and average blood pressure.
  \item After controlling for age and average blood pressure, the effect of BMI 
    on disease progression is different for men and women.
  \end{enumerate}
  \vb
  We can represent these beliefs with a moderated regression model:
  \begin{align*}
    Y_{prog} = \beta_0 + \beta_1 X_{BMI} + \beta_2 X_{sex} + \beta_3 X_{age} + \beta_4 X_{BP} + \beta_5 X_{BMI} X_{sex} + \varepsilon
  \end{align*}
  
\end{frame}

\watermarkoff %----------------------------------------------------------------%

\begin{frame}[fragile]{Data Modeling Example}
  
  We can use R to fit our model to some patient data:
  
<<messages = FALSE>>=
library(rockchalk)

## Load the data:
dataDir <- "../data/"
dDat    <- readRDS(paste0(dataDir, "diabetes.rds"))

## Fit the regression model:
fit <- lm(progress ~ bmi * sex + age + bp, data = dDat)
@ 

\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}[fragile]{Data Modeling Example}

<<>>=
partSummary(fit, -c(1, 2))
@ 

\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}[fragile]{Data Modeling Example}
  
  We can do a simple slopes analysis to test the group-specific effects of BMI 
  on disease progression:
  
<<include = FALSE>>=
psOut <- plotSlopes(fit, plotx = "bmi", modx = "sex")
tsOut <- testSlopes(psOut)
@ 

<<eval = FALSE>>=
psOut <- plotSlopes(fit, plotx = "bmi", modx = "sex")
tsOut <- testSlopes(psOut)
@ 

<<>>=
tsOut$hypotests[ , -1]
@ 

\end{frame}

\watermarkon %-----------------------------------------------------------------%

\begin{frame}[fragile]{Data Modeling Example}

  We can also visualize the simple slopes:
  
<<echo = FALSE, out.width = '6.5cm'>>=
plotSlopes(fit, plotx = "bmi", modx = "sex")
@

\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}{Algorithmic Modeling Example}
  
  Suppose we want to find the best predictors of disease progression among the
  variables contained in our dataset:
  \begin{columns}
    \begin{column}{0.4\textwidth}
      \begin{itemize}
      \item Age
      \item BMI
      \item Blood Pressure
      \item Blood Glucose
      \item Sex
      \end{itemize}
      
    \end{column}
    \begin{column}{0.4\textwidth}
      
      \begin{itemize}
      \item Total Cholesterol
      \item LDL Cholesterol
      \item HDL Cholesterol
      \item Triglycerides
      \item Lamorigine 
      \end{itemize}
      
    \end{column}
  \end{columns}
  \va
  We could try \emph{best-subset selection}.
  \begin{itemize}
  \item Fit a series of regression models wherein disease progression is
    predicted by all possible subsets of X variables.
  \item Choose the set of X variables that minimizes the prediction error.
  \end{itemize}
  
\end{frame}

\watermarkoff %----------------------------------------------------------------%

\begin{frame}[fragile]{Algorithmic Modeling Example}
  
<<>>=
library(leaps)

## Save the predictor variables' names:
xNames <- grep(pattern = "progress", 
               x       = colnames(dDat), 
               invert  = TRUE, 
               value   = TRUE)

## Train the models:
fit <- regsubsets(x     = progress ~ ., 
                  data  = dDat, 
                  nvmax = ncol(dDat) - 1)

## Summarize the results:
sum <- summary(fit)
@ 

\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}[fragile]{Algorithmic Modeling Example}

<<>>=
sum$outmat
@ 

\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}[fragile]{Algorithmic Modeling Example}

<<>>=
## Variables selected by BIC:
xNames[with(sum, which[which.min(bic), -1])]

## Variables selected by Adjusted R^2:
xNames[with(sum, which[which.max(adjr2), -1])]

## Variables selected by Mallow's Cp:
xNames[with(sum, which[which.min(cp), -1])]
@

\end{frame}

\watermarkon %-----------------------------------------------------------------%

\begin{frame}{Algorithmic Modeling Example}

  The results seem to be highly sensitive to the error measure. What should we
  do?
  \pause
  \begin{itemize}
  \item We could pick our favorite error measure and use its results.
  \item We could throw our hands up in defeat and quit.
  \item We could look at the results and pick the answer we like best.
    \begin{itemize}
    \item The previous two suggestions are sub-optimal, but this one is
      actually unethical. Don't do this!
    \end{itemize}
  \end{itemize}
  \pause
  \vb
  If we think like a data scientist and get creative, we don't need to settle
  for these ambiguous results.
  \begin{itemize}
  \item We could implement a more robust method of calculating prediction error 
    like \emph{K-fold cross validation}.
  \item We can use resampling methods to quantify uncertainty in the variable
    selection process.
  \end{itemize}
  
\end{frame}

\watermarkoff %----------------------------------------------------------------%

\begin{frame}[fragile]{Algorithmic Modeling Example}

<<>>=
bic <- r2 <- cp <- matrix(NA, 100, ncol(dDat) - 1)
for(rp in 1 : 100) {
    ## Resample the data:
    tmp <- dDat[sample(1 : nrow(dDat), nrow(dDat), TRUE), ]
    
    ## Train the models:
    fit <- regsubsets(x     = progress ~ ., 
                      data  = tmp, 
                      nvmax = ncol(tmp) - 1)
    sum <- summary(fit)
    
    ## Save the optimal selections:
    bic[rp, ] <- with(sum, which[which.min(bic), -1])
    r2[rp, ]  <- with(sum, which[which.max(adjr2), -1])
    cp[rp, ]  <- with(sum, which[which.min(cp), -1])
}
@ 

<<echo = FALSE>>=
colnames(bic) <- colnames(r2) <- colnames(cp) <- colnames(sum$which)[-1]
@

\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}[fragile]{Algorithmic Modeling Example}

<<size = 'scriptsize'>>=
colMeans(bic)
colMeans(r2)
colMeans(cp)
@

\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}[fragile]{Algorithmic Modeling Example}

<<>>=
## Find the best subset via majority vote:
votes <- colMeans(rbind(bic, r2, cp)); round(votes, 3)

preds <- xNames[votes > 0.5]; preds

## Fit the winning model to the original data:
form <- paste0("progress ~ ", 
               paste(preds, collapse = " + ")
               )
fit  <- lm(form, data = dDat)
@ 

\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}[fragile]{Algorithmic Modeling Example}

<<>>=
partSummary(fit, -c(1, 2))
@   

\end{frame}

%------------------------------------------------------------------------------%

\comment{%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[shrink = 5]{Statistics for Data Science}

  Data scientists must scrutinize large numbers of data and extract useful
  knowledge.
  \vb
  \begin{itemize}
  \item Data contain raw \emph{information}.
    \va
  \item To convert this information into actionable \emph{knowledge}, data
    scientists apply various data analytic techniques.
    \va
  \item When presenting the results of such analyses, data scientists must be
    careful not to over-state their findings.
    \va
  \item Too much confidence in an uncertain finding can lead to very poor
    outcomes.  
    \va
  \item Statistics offers us a way to protect ourselves from \emph{ourselves}.
  \end{itemize}

\end{frame}
}%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\watermarkon %-----------------------------------------------------------------%

\begin{frame}{Data Science for the Social and Behavioral Sciences}
  
  Social and behavioral scientists specialize in the types of problems for which
  a data science approach is most beneficial.
  \vb
  \begin{itemize}
  \item Social systems are messy, noisy, and chaotic.
    \vb
  \item Social systems are usually complex systems.
    \vb
  \item Social scientific constructs tend to be difficult to measure.
    \vb
  \item Human behavior produces a lot of data.
    \vb
  \item Many layers of uncertainty open the door for a host of poor/unethical
    research practices.
  \end{itemize}
  
\end{frame}
 
%------------------------------------------------------------------------------%
\comment{%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{Looking Ahead}

  \begin{itemize}
  \item Next time, we will review \emph{linear regression modeling}.\\
    \vb
    \begin{itemize}
    \item I will also distribute and explain the first group assignment.
    \end{itemize}
    \va
  \item We have our first lab meeting on Friday (no lab on Wednesday).
    \vb
    \begin{itemize}
    \item Work through ``A (very) short introduction to R'' on your own.
    \end{itemize}
  \end{itemize}
  
\end{frame}
}%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%------------------------------------------------------------------------------%

\begin{frame}[allowframebreaks]{References}

  \bibliographystyle{apacite}
  \bibliography{../../../bibtexStuff/idsRefs.bib}

\end{frame}

%------------------------------------------------------------------------------%

\end{document}

