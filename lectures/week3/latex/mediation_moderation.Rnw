%%% Title:    TCSM: Mediation & Moderation
%%% Author:   Kyle M. Lang
%%% Created:  2016-XX-XX
%%% Modified: 2023-08-25

\documentclass[10pt]{beamer}
\usetheme{Utrecht}

\usepackage{graphicx}
\usepackage[natbibapa]{apacite}
\usepackage[libertine]{newtxmath}
\usepackage{fancybox}
\usepackage{booktabs}
\usepackage{relsize}

\newcommand{\eqit}[1]{\textrm{\textit{#1}}}
\newcommand{\pkg}[1]{\textbf{#1}}
\newcommand{\src}[1]{\texttt{#1}}
\newcommand{\rmsc}[1]{\textrm{\textsc{#1}}}

\title{Mediation \& Moderation}
\subtitle{Theory Construction and Statistical Modeling}
\author{Kyle M. Lang}
\institute{Department of Methodology \& Statistics\\Utrecht University}
\date{}

\begin{document}

<<setup, include = FALSE, cache = FALSE>>=
set.seed(235711)

dataDir <- "../data/"

library(knitr)
library(ggplot2)
library(MASS)
library(DAAG)
library(xtable)
library(MLmetrics)
library(dplyr)
library(mvtnorm)
library(lavaan)

source("../../../code/supportFunctions.R")

uu_yellow <- rgb(255, 205, 0, 255, max = 255)
uu_red <- rgb(192, 10, 53, 255, max = 255)
uu_cream <- rgb(250, 230, 171, 255, max = 255)
uu_orange <- rgb(243, 150, 94, 255, max = 255)
uu_burgundy <- rgb(170, 21, 85, 255, max = 255)
uu_brown <- rgb(110, 59, 35, 255, max = 255)
uu_green <- rgb(36, 167, 147, 255, max = 255)
uu_blue <- rgb(82, 135, 198, 255, max = 255)
uu_darkblue <- rgb(0, 18, 64, 255, max = 255)
uu_purple <- rgb(91, 33, 130, 255, max = 255)

options(width = 80)
opts_chunk$set(size = "footnotesize",
               fig.align = "center",
               fig.path = "figure/mediation_moderation-",
               message = FALSE,
               comment = "")
knit_theme$set('edit-kwrite')
@

%------------------------------------------------------------------------------%

\begin{frame}[t,plain]
  \titlepage
\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}{Outline}
  \tableofcontents
\end{frame}

%%%--------------------------------------------------------------------------%%%

\section{Mediation}

%%%--------------------------------------------------------------------------%%%

\begin{frame}{Mediation vs. Moderation}

  What do we mean by \emph{mediation} and \emph{moderation}?\\
  \va
  Mediation and moderation are types of hypotheses, not statistical methods or
  models.
  \begin{itemize}
    \item Mediation tells us \emph{how} one variable influences another.
      \vb
    \item Moderation tells us \emph{when} one variable influences another.
  \end{itemize}

\end{frame}

%%%--------------------------------------------------------------------------%%%

\begin{frame}{Contextualizing Example}

  Say we wish to explore the process underlying exercise habits.\\
  \va
  Our first task is to operationalize ``exercise habits''
  \begin{itemize}
    \item DV: Hours per week spent in vigorous exercise (\emph{exerciseAmount}).
  \end{itemize}
  \va
  We may initial ask: what predicts devoting more time to exercise?
  \begin{itemize}
    \item IV: Concerns about negative health outcomes (\emph{healthConcerns}).
  \end{itemize}

\end{frame}

%%%--------------------------------------------------------------------------%%%

\begin{frame}{Focal Effect Only}

  The $healthConcerns \rightarrow exerciseAmount$ relation is our \emph{focal
  effect}

  \begin{figure}
    \includegraphics[width=\textwidth]{../figures/focalEffectDiagram.pdf}
  \end{figure}

  \begin{itemize}
    \item Mediation and moderation both attempt to describe the focal effect in
      more detail.
      \vb
    \item We always begin by hypothesizing a focal effect.
  \end{itemize}

\end{frame}

\watermarkoff %%%------------------------------------------------------------%%%

\begin{frame}{The Mediation Hypothesis}

  A mediation analysis will attempt to describe how health concerns affect
  amount of exercise.
  \va
  \begin{itemize}
    \item The \emph{how} is operationalized in terms of intermediary variables.
      \va
    \item Mediator: Motivation to improve health (\emph{motivation}).
  \end{itemize}

  \vx{-18}

  \begin{figure}
    \includegraphics[width=0.8\textwidth]{../figures/mediationDiagram.pdf}
  \end{figure}

\end{frame}

%%%--------------------------------------------------------------------------%%%

\begin{frame}{Moderation Hypothesis}

  A moderation hypothesis will attempt to describe when health concerns affect
  amount of exercise.
  \va
  \begin{itemize}
    \item The \emph{when} is operationalized in terms of interactions between
      the focal predictor and contextualizing variables
      \va
    \item Moderator: Sense of personal agency relating to physical health
      (\emph{agency}).
  \end{itemize}

  \vx{-18}

  \begin{figure}
    \includegraphics[width=0.8\textwidth]{../figures/moderationDiagram.pdf}
  \end{figure}

\end{frame}

%%%--------------------------------------------------------------------------%%%

\subsection{Indirect Effects}

%%%--------------------------------------------------------------------------%%%

\begin{frame}{Path Diagrams}

  \begin{figure}
    \includegraphics[width=\textwidth]{../figures/simpleMediationPathDiagram.pdf}
  \end{figure}

\end{frame}

\watermarkon %%%-------------------------------------------------------------%%%

\begin{frame}{Necessary Equations}

  To get all the pieces of the preceding diagram using OLS regression, we'll
  need to fit three separate models.

  \begin{align}
    Y &= i_1 + cX + e_1 \label{eq1}\\
    Y &= i_2 + c'X + bM + e_2 \label{eq2}\\
    M &= i_3 + aX + e_3 \label{eq3}
  \end{align}

  \begin{itemize}
    \item Equation \ref{eq1} gives us the total effect ($c$).
      \vb
    \item Equation \ref{eq2} gives us the direct effect ($c'$) and the partial
      effect of the mediator on the outcome ($b$).
      \vb
    \item Equation \ref{eq3} gives us the effect of the input on the outcome ($a$).
  \end{itemize}

\end{frame}

%%%--------------------------------------------------------------------------%%%

\begin{frame}{Two Measures of Indirect Effect}

  Indirect effects can be quantified in two different ways:
  \begin{align}
    IE_{diff} &= c - c'\\
    IE_{prod} &= a \times b
  \end{align}
  $IE_{diff}$ and $IE_{prod}$ are equivalent in simple mediation.
  \va
  \begin{itemize}
    \item Both give us information about the proportion of the total
      effect that is transmitted through the intermediary variable.
      \vb
    \item $IE_{prod}$ provides a more direct representation of the
      actual pathway we're interested in testing.
      \vb
    \item $IE_{diff}$ gets at our desired hypothesis indirectly.
  \end{itemize}

\end{frame}

%%%--------------------------------------------------------------------------%%%

\subsection{Causal Steps Approach}

%%%--------------------------------------------------------------------------%%%

\begin{frame}{The Causal Steps Approach}

  \citet[][p. 1176]{baronKenny:1986} describe three/four conditions
  as being sufficient to demonstrate statistical ``mediation.''
  \va
  \begin{enumerate}
    \item Variations in levels of the independent variable significantly
      account for variations in the presumed mediator (i.e., Path
      \emph{a}).
      \begin{itemize}
        \item Need a significant $a$ path.
      \end{itemize}
      \vb
    \item Variations in the mediator significantly account for variations
      in the dependent variable (i.e., Path \emph{b}).
      \begin{itemize}
        \item Need a significant $b$ path.
      \end{itemize}
      \vb
    \item When Paths \emph{a} and \emph{b} are controlled, a previously
      significant relation between the independent and dependent
      variables is no longer significant.
      \begin{itemize}
        \item Need a significant total effect
        \item The direct effect must be ``less'' than the total effect
      \end{itemize}
  \end{enumerate}

\end{frame}

\watermarkoff %%%------------------------------------------------------------%%%

\begin{frame}{Example Process Model}

  Consider the following process.

  \begin{figure}
    \includegraphics[width=0.8\textwidth]{../figures/adamsKlpsExample1_two_models.pdf}
  \end{figure}

\end{frame}

%%%--------------------------------------------------------------------------%%%

\begin{frame}[fragile]{Causal Steps Example}

<<>>=
## Load some data:
dat1 <- readRDS("../data/adamsKlpsScaleScore.rds")

## Check pre-conditions:
mod1 <- lm(policy ~ polAffil, data = dat1)
mod2 <- lm(policy ~ sysRac, data = dat1)
mod3 <- lm(sysRac ~ polAffil, data = dat1)

## Partial out the mediator's effect:
mod4 <- lm(policy ~ sysRac + polAffil, data = dat1)
@

\end{frame}

%%%--------------------------------------------------------------------------%%%

\begin{frame}[fragile]{Causal Steps Example}

<<>>=
summary(mod1)
@

\end{frame}

%%%--------------------------------------------------------------------------%%%

\begin{frame}[fragile]{Causal Steps Example}

<<>>=
summary(mod2)
@

\end{frame}

%%%--------------------------------------------------------------------------%%%

\begin{frame}[fragile]{Causal Steps Example}

<<>>=
summary(mod3)
@

\end{frame}

%%%--------------------------------------------------------------------------%%%

\begin{frame}[fragile]{Causal Steps Example}

<<>>=
summary(mod4)
@

\end{frame}

%%%--------------------------------------------------------------------------%%%

\begin{frame}[fragile]{Causal Steps Example}

<<>>=
## Extract important parameter estimates:
a      <- coef(mod3)["polAffil"]
b      <- coef(mod4)["sysRac"]
c      <- coef(mod1)["polAffil"]
cPrime <- coef(mod4)["polAffil"]

## Compute indirect effects:
ieDiff <- unname(c - cPrime)
ieProd <- unname(a * b)

ieDiff
ieProd
@

\end{frame}

\watermarkon %%%-------------------------------------------------------------%%%

\subsection{Sobel's Test}

%%%--------------------------------------------------------------------------%%%

\begin{frame}{Sobel's Z}

  In the previous example, do we have a \emph{significant} indirect effect?
  \va
  \begin{itemize}
    \item The direct effect is ``substantially'' smaller than the total effect,
      but is the difference statistically significant?
      \vb
    \item \citet{sobel:1982} developed an asymptotic standard error for $IE_{prod}$
      that we can use to assess this hypothesis.
  \end{itemize}
  \begin{align}
    SE_{sobel} &= \sqrt{a^2 \times SE_b^2 + b^2 \cdot SE_a^2}\\
    Z_{sobel} &= \frac{ab}{SE_{sobel}}\\
    95\% CI_{sobel} &= ab \pm 1.96 \times SE_{sobel}
  \end{align}

\end{frame}

\watermarkoff %%%------------------------------------------------------------%%%

\begin{frame}[fragile]{Sobel Example}

<<>>=
## SE:
seA <- (mod3 %>% vcov() %>% diag() %>% sqrt())["polAffil"]
seB <- (mod4 %>% vcov() %>% diag() %>% sqrt())["sysRac"]

se <- sqrt(b^2 * seA^2 + a^2 * seB^2) %>% unname()

## z-score:
(z <- ieProd / se)

## p-value:
(p <- 2 * pnorm(z, lower = FALSE))

## 95% CI:
c(ieProd - 1.96 * se, ieProd + 1.96 * se)
@

\end{frame}

%%%--------------------------------------------------------------------------%%%

\begin{frame}{Recall our Basic Path Diagram}

  \begin{figure}
    \includegraphics[width=\textwidth]{../figures/simpleMediationPathDiagram.pdf}
  \end{figure}

\end{frame}

\watermarkon %%%-------------------------------------------------------------%%%

\begin{frame}{Two Measures of Indirect Effect}

  Recall the two definitions of an indirect effect:
  \begin{align}
    IE_{diff} &= c - c'\\
    IE_{prod} &= a \times b
  \end{align}

  It pays to remember a few key points:
  \vc
  \begin{itemize}
    \item $IE_{diff}$ and $IE_{prod}$ are equivalent in simple
      mediation.
      \vb
    \item $IE_{diff}$ is only an indirect indication of $IE_{prod}$.
      %  \vb
      %\item A significant indirect effect can exist without a significant
      %  total effect.
      \vb
    \item If we only care about the indirect effect, then we don't need to worry
      about the total effect.
  \end{itemize}
  \pause
  \vb
  These points imply something interesting:
  \vc
  \begin{itemize}
    \item We don't need to estimate $c$!
  \end{itemize}

\end{frame}

\watermarkoff %%%------------------------------------------------------------%%%

\begin{frame}{Simplifying our Path Diagram}

  \rmsc{Question:} If we don't care about directly estimating $c$, how can we
  simplify this diagram?

  \begin{figure}
    \includegraphics[width=0.8\textwidth]{../figures/simpleMediationPathDiagram.pdf}
  \end{figure}

\end{frame}

%%%--------------------------------------------------------------------------%%%

\begin{frame}{Simplifying our Path Diagram}

  \rmsc{Answer:} We don't fit the upper model.

  \begin{figure}
    \includegraphics[width=0.8\textwidth]{../figures/mediationTriadPathDiagram.pdf}
  \end{figure}

\end{frame}

%%%--------------------------------------------------------------------------%%%

\begin{frame}{Why Path Analysis?}

  \begin{figure}
    \includegraphics[width=\textwidth]{../figures/multipleMediationPathDiagrams.pdf}
  \end{figure}

\end{frame}

%%%--------------------------------------------------------------------------%%%

\begin{frame}{Example}

  Let's revisit the above example using path analysis in \pkg{lavaan}.

  \begin{figure}
    \includegraphics[width=\textwidth]{../figures/adamsKlpsExample1PathDiagram.pdf}
  \end{figure}

\end{frame}

%%%--------------------------------------------------------------------------%%%

\begin{frame}[fragile]{Example}

<<warning = FALSE>>=
## Load the lavaan package:
library(lavaan)

## Specify the basic path model:
mod1 <- '
policy ~ 1 + sysRac + polAffil
sysRac ~ 1 + polAffil
'

## Estimate the model:
out1 <- sem(mod1, data = dat1)
@

\end{frame}

%%%--------------------------------------------------------------------------%%%

\begin{frame}[fragile]{Example}

<<>>=
## Look at the results:
partSummary(out1, 7:9)
@

\end{frame}

%%%--------------------------------------------------------------------------%%%

\begin{frame}[fragile]{Example}

<<>>=
## Include the indirect effect:
mod2 <- '
policy ~ 1 + b*sysRac + polAffil
sysRac ~ 1 + a*polAffil

ab := a*b # Define a parameter for the indirect effect
'

## Estimate the model:
out2 <- sem(mod2, data = dat1)
@

\end{frame}

%%%--------------------------------------------------------------------------%%%

\begin{frame}[fragile]{Example}

<<>>=
## Look at the results:
partSummary(out2, 7:8)
@

\end{frame}

%%%--------------------------------------------------------------------------%%%

\begin{frame}[fragile]{Example}

<<>>=
partSummary(out2, 9:10)
@

\end{frame}

%%%--------------------------------------------------------------------------%%%

\begin{frame}[fragile]{Example}

<<>>=
## We can also get CIs:
parameterEstimates(out2, zstat = FALSE, pvalue = FALSE, ci = TRUE)
@

\end{frame}

%%%--------------------------------------------------------------------------%%%

\begin{frame}{Results}

  \begin{figure}
    \begin{center}
      \includegraphics[width = \textwidth]{../figures/adamsKlpsExample1PathDiagramWithValues.pdf}
    \end{center}
  \end{figure}

\end{frame}

\watermarkon %%%-------------------------------------------------------------%%%

\begin{frame}{We're not there yet...}

  Path analysis allows us to directly model complex (and simple) relations, but
  the preceding example still suffers from a considerable limitation.
  \vb
  \begin{itemize}
    \item The significance test for the indirect effect is still conducted with
      the Sobel Z approach.
  \end{itemize}
  \va
  Path analysis (or full SEM) doesn't magically get around distributional
  problems associated with Sobel's Z test.\\
  \vb
  \begin{itemize}
    \item To get a robust significance test of the indirect effect, we need to use
      \emph{bootstrapping}.
  \end{itemize}

\end{frame}

%%%--------------------------------------------------------------------------%%%

\subsection{Bootstrapping}

%%%--------------------------------------------------------------------------%%%

\begin{frame}{Bootstrapping}

  Bootstrapping was introduced by \citet{efron:1979} as a tool for
  non-parametric inference.
  \vb
  \begin{itemize}
    \item Traditional inference requires that we assume a parametric sampling
      distribution for our focal parameter.
      \vb
    \item We need to make such an assumption to compute the standard errors we
      require for inferences.
      \vb
    \item If we cannot safely make these assumptions, we can use bootstrapping.
  \end{itemize}

\end{frame}

%%% --------------------------------------------------------------------------%%%

\begin{frame}{Bootstrapping}

  Assume our observed data $Data_0$ represent the population and:
  \vb
  \begin{enumerate}
    \item Sample rows of $Data_0$, with replacement, to create $B$ new samples
      $\{Data_b\}$.
      \vb
    \item Calculate our focal statistic on each of the $B$ bootstrap samples. \label{calcStatsStep}
      \vb
    \item Make inferences based on the empirical distribution of the $B$
      estimates calculated in Step \ref{calcStatsStep}
  \end{enumerate}

\end{frame}

\watermarkoff %%%------------------------------------------------------------%%%

\begin{frame}{Bootstrapping}

  \begin{figure}
    \begin{center}
      \includegraphics[width=\textwidth]{../figures/bootstrappingDiagram.pdf}
    \end{center}
  \end{figure}

\end{frame}

\watermarkon %%%-------------------------------------------------------------%%%

\begin{frame}[allowframebreaks]{Example}

  Suppose I'm on the lookout for a retirement location. Since I want to relax in
  my old-age, I'm concerned with ensuring a low probability of dragon attacks,
  so I have a few salient considerations:
  \vb
  \begin{itemize}
    \item Shooting for a location with no dragons, whatsoever, is a fools errand
      (since dragons are, obviously, ubiquitous).
      \vb
    \item I merely require a location that has at least two times as many
      dragon-free days as other kinds.
  \end{itemize}

  \pagebreak

  I've been watching several candidate locales over the course of my (long and
  illustrious) career, and I'm particularly hopeful about one quiet hamlet in
  the Patagonian highlands.
  \vb
  \begin{itemize}
    \item To ensure that my required degree of dragon-freeness is met, I'll use
      the \emph{Dragon Risk Index} (DRI):
      \begin{align*}
        DRI = \textit{Median}\left( \frac{\text{Dragon-Free Days}}{\text{Dragonned Days}} \right)
      \end{align*}
  \end{itemize}

\end{frame}

\watermarkoff %%%------------------------------------------------------------%%%

\begin{frame}[fragile, allowframebreaks]{Example}

<<>>=
## Load some useful packages:
library(dplyr)
library(magrittr)

## Read in the observed data:
rawData <- readRDS("../data/daysData.rds")

## Compute the observed test statistic:
(obsDRI <- with(rawData, median(goodDays / badDays)))
@

\pagebreak

<<boot1, cache = TRUE>>=
## Define the number of bootstrap samples:
nSams <- 5000 

## Set a seed for the RNG:
set.seed(235711)

## Bootstrap the DRI statistic:
bootDRI <- rep(NA, nSams)
for(b in 1:nSams) 
  bootDRI[b] <- rawData %>% 
    slice_sample(prop = 1, replace = TRUE) %$% # Resample the data
    median(goodDays / badDays)                 # Calculate the DRI
@

\end{frame}

%%%--------------------------------------------------------------------------%%%

\begin{frame}[fragile]{Example}

<<echo = FALSE, out.width = "65%">>=
gg0(x = bootDRI, points = FALSE) +
geom_histogram(aes(y = after_stat(density)), fill = "lightgray", color = "black") +
geom_density(color = "red") +
xlab("Dragon Risk Index") +
ylab("Density") +
ggtitle("Empirical Sampling Distribution of DRI")
@

\end{frame}

%%%--------------------------------------------------------------------------%%%

\begin{frame}[fragile]{Example}

  To see if I can be confident in the dragon-freeness of my potential home, I'll
  summarize the preceding distribution with a (one-tailed) percentile confidence
  interval:
<<>>=
## Compute the 95% bootstrapped percentile CI:
quantile(bootDRI, c(0.05, 1.0))
@

Since we have a directional hypothesis, the upper bound of this interval is a
bit misleading.
<<>>=
max(bootDRI)
qnorm(1.0, mean(bootDRI), sd(bootDRI))
@

\end{frame}

\watermarkon %%%-------------------------------------------------------------%%%

\begin{frame}{Bootstrapped Inference for Indirect Effects}

  We can apply the same procedure to testing the indirect effect.
  \vb
  \begin{itemize}
    \item The problem with Sobel's Z is exactly the type of issue for which
      bootstrapping was designed
      \vc
      \begin{itemize}
        \item We don't know a reasonable finite-sample sampling distribution for the
          $ab$ parameter.
      \end{itemize}
      \vb
    \item Bootstrapping will allow us to construct an empirical sampling
      distribution for $ab$ and construct confidence intervals for inference.
  \end{itemize}

\end{frame}

%%%--------------------------------------------------------------------------%%%

\begin{frame}{Bootstrapped Inference for Indirect Effects}

  \rmsc{Procedure:}
  \vc
  \begin{enumerate}
    \item Resample our observed data with replacement
      \vc
    \item Fit our hypothesized path model to each bootstrap sample
      \vc
    \item Store the value of $ab$ that we get each time
      \vc
    \item Summarize the empirical distribution of $ab$ to make inferences
  \end{enumerate}

\end{frame}

\watermarkoff %%%------------------------------------------------------------%%%

\begin{frame}[fragile]{Example}

<<boot2, cache = TRUE>>=
abVec <- rep(NA, nSams)
for(i in 1:nSams) 
  abVec[i] <- dat1 %>%
    slice_sample(prop = 1, replace = TRUE) %>% # Resample the data
    sem(mod2, data = .) %>%                    # Fit the model
    coef() %>%                                 # Extract estimates
    extract(c("a", "b")) %>%                   # Isolate a and b
    prod()                                     # Calculate IE
@

\end{frame}

%%%--------------------------------------------------------------------------%%%

\begin{frame}[fragile]{Example}

<<echo = FALSE, out.width = "65%">>=
gg0(x = abVec, points = FALSE) +
geom_histogram(aes(y = after_stat(density)),
               fill = "lightgray", 
               color = "black") +
geom_density(color = "red") +
ggtitle("Empirical Sampling Distribution of a*b") +
xlab("a*b")
@

\end{frame}

%%%--------------------------------------------------------------------------%%%

\begin{frame}[fragile]{Example}

  \begin{columns}
    \begin{column}{0.45\textwidth}

<<>>=
## Calculate the 95% CI:
quantile(abVec, c(0.025, 0.975))
@

    \end{column}
    \begin{column}{0.55\textwidth}

<<echo = FALSE>>=
ci <- quantile(abVec, c(0.025, 0.975))

gg0(x = abVec, points = FALSE) +
  geom_histogram(aes(y    = after_stat(density),
                     fill = after_stat(x <= ci[1] | x >= ci[2])
                     ),
               color = "black") +
scale_fill_manual(values = c("lightgray", uu_orange)) +
theme(legend.position = "none") +
geom_density(color = "red") +
ggtitle("Empirical Sampling Distribution of a*b") +
xlab("a*b")
@

    \end{column}
  \end{columns}

\end{frame}

%%%--------------------------------------------------------------------------%%%

\begin{frame}[fragile]{Example}

<<boot3, cache = TRUE>>=
## Much more parsimoniously:
bootOut2 <- sem(mod2, data = dat1, se = "boot", bootstrap = nSams)

parameterEstimates(bootOut2, zstat = FALSE, pvalue = FALSE)
@

\end{frame}

\watermarkon %%%-------------------------------------------------------------%%%

\sectionslide{Moderation}

\watermarkoff %%%------------------------------------------------------------%%%

\begin{frame}{Refresher: Moderation Hypothesis}

  A moderation hypothesis will attempt to describe when health concerns affect
  amount of exercise.
  \va
  \begin{itemize}
    \item The \emph{when} is operationalized in terms of interactions between
      the focal predictor and contextualizing variables
      \va
    \item Moderator: Sense of personal agency relating to physical health
      (\emph{agency}).
  \end{itemize}

  \vx{-18}

  \begin{figure}
    \includegraphics[width=0.8\textwidth]{../figures/moderationDiagram.pdf}
  \end{figure}

\end{frame}

\watermarkon %%%-------------------------------------------------------------%%%

\begin{frame}{Equations}

  In additive MLR, we might have the following equation:
  \begin{align*}
    Y = \beta_0 + \beta_1X + \beta_2Z + \varepsilon
  \end{align*}
  This additive equation assumes that $X$ and $Z$ are independent predictors of 
  $Y$.\\
  \va
  When $X$ and $Z$ are independent predictors, the following are true:
  \vb
  \begin{itemize}
    \item $X$ and $Z$ \emph{can} be correlated.
      \vb
    \item $\beta_1$ and $\beta_2$ are \emph{partial} regression
      coefficients.
      \vb
    \item \color{uured}The effect of $X$ on $Y$ is the same at \textbf{all levels} 
      of $Z$, and the effect of $Z$ on $Y$ is the same at \textbf{all levels} 
      of $X$.
  \end{itemize}

\end{frame}

\watermarkoff %----------------------------------------------------------------%

\begin{frame}{Additive Regression}

  The effect of $X$ on $Y$ is the same at \textbf{all levels} of $Z$.

  \begin{columns}
    \begin{column}{0.45\textwidth}
      \includegraphics[width = 1.1\textwidth]{../figures/3d_data_plot}
    \end{column}

    \begin{column}{0.1\textwidth}
      \begin{center}\Huge{$\rightarrow$}\end{center}
    \end{column}

    \begin{column}{0.45\textwidth}
      \includegraphics[width = 1.1\textwidth]{../figures/response_surface_plot0}
    \end{column}
  \end{columns}

\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}{Moderated Regression}

  The effect of $X$ on $Y$ varies \textbf{as a function} of $Z$.

  \begin{columns}
    \begin{column}{0.45\textwidth}
      \includegraphics[width = 1.1\textwidth]{../figures/3d_data_plot}
    \end{column}

    \begin{column}{0.1\textwidth}
      \begin{center}\Huge{$\rightarrow$}\end{center}
    \end{column}

    \begin{column}{0.45\textwidth}
      \includegraphics[width = 1.1\textwidth]{../figures/response_surface_plot}
    \end{column}
  \end{columns}

\end{frame}

\watermarkon %-----------------------------------------------------------------%

\begin{frame}{Equations}

  The following derivation is adapted from \citet{hayes:2022}.
  \vb
  \begin{itemize}
    \item When testing moderation, we hypothesize that the effect of $X$ on $Y$
      varies as a function of $Z$.
      \vb
    \item We can represent this concept with the following equation:
      \begin{align}
        Y = \beta_0 + f(Z)X + \beta_2Z + \varepsilon \label{fEq}
      \end{align}
      \vx{-8}
      \pause
    \item If we assume that $Z$ linearly (and deterministically) affects the
      relationship between $X$ and $Y$, then we can take:
      \begin{align}
        f(Z) = \beta_1 + \beta_3Z \label{ssEq}
      \end{align}
  \end{itemize}

\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}{Equations}

  \begin{itemize}
    \item Substituting Equation \ref{ssEq} into Equation \ref{fEq} leads to:
      \begin{align*}
        Y = \beta_0 + (\beta_1 + \beta_3Z)X + \beta_2Z + \varepsilon
      \end{align*}
      \pause
    \item Which, after distributing $X$ and reordering terms, becomes:
      \begin{align*}
        Y = \beta_0 + \beta_1X + \beta_2Z + \beta_3XZ + \varepsilon
      \end{align*}
  \end{itemize}

\end{frame}

%%%--------------------------------------------------------------------------%%%

\begin{frame}{Conceptual vs. Analytic Diagrams}

  \begin{columns}
    \begin{column}{0.5\textwidth}

      \begin{figure}
        \includegraphics[width=\textwidth]{../figures/modConcept.pdf}
      \end{figure}

    \end{column}
    \begin{column}{0.5\textwidth}

      \only<1>{
        \begin{figure}
          \includegraphics[width=\textwidth]{../figures/modAnalytic1.pdf}
        \end{figure}
      }
      \only<2>{
        \begin{figure}
          \includegraphics[width=\textwidth]{../figures/modAnalytic2.pdf}
        \end{figure}
      }

    \end{column}
  \end{columns}

\end{frame}

%%%--------------------------------------------------------------------------%%%

\subsection{Testing Moderation}

%%%--------------------------------------------------------------------------%%%

\begin{frame}{Testing Moderation}

  Now, we have an estimable regression model that quantifies the linear
  moderation we hypothesized.
  \vb
  \begin{center}\ovalbox{$Y = \beta_0 + \beta_1X + \beta_2Z + \beta_3XZ +
  \varepsilon$}\end{center}
  \vc
  \begin{itemize}
    \item To test for significant moderation, we simply need to test the
      significance of the interaction term, $XZ$.
      \begin{itemize}
        \item Check if $\hat{\beta}_3$ is significantly different from zero.
      \end{itemize}
  \end{itemize}

\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}{Interpretation}

  Given the following equation:
  \begin{align*}
    Y = \hat{\beta}_0 + \hat{\beta}_1X + \hat{\beta}_2Z + \hat{\beta}_3XZ +
    \hat{\varepsilon}
  \end{align*}
  \vx{-16}
  \begin{itemize}
    \item $\hat{\beta}_3$ quantifies the effect of $Z$ on the focal effect (the $X
      \rightarrow Y$ effect).
      \vc
      \begin{itemize}
        \item For a unit change in $Z$, $\hat{\beta}_3$ is the expected change in
          the effect of $X$ on $Y$.
      \end{itemize}
      \vb
    \item $\hat{\beta}_1$ and $\hat{\beta}_2$ are \emph{conditional effects}.
      \vc
      \begin{itemize}
        \item Interpreted where the other predictor is zero.
          \vc
        \item For a unit change in $X$, $\hat{\beta}_1$ is the expected change in
          $Y$, when $Z = 0$.
          \vc
        \item For a unit change in $Z$, $\hat{\beta}_2$ is the expected change in
          $Y$, when $X = 0$.
      \end{itemize}
  \end{itemize}

\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}{Example}

  Looking at the \emph{diabetes} dataset.
  \va
  \begin{itemize}
    \item We suspect that patients' BMIs are predictive of their average blood
      pressure.
      \va
    \item We further suspect that this effect may be differentially expressed
      depending on the patients' LDL levels.
  \end{itemize}

\end{frame}

%%%--------------------------------------------------------------------------%%%

\begin{frame}{Diagrams}

  \begin{columns}
    \begin{column}{0.5\textwidth}

      \begin{figure}
        \includegraphics[width=\textwidth]{../figures/diabetes_example_concept.pdf}
      \end{figure}

    \end{column}
    \begin{column}{0.5\textwidth}

      \begin{figure}
        \includegraphics[width=\textwidth]{../figures/diabetes_example_analytic.pdf}
      \end{figure}

    \end{column}
  \end{columns}

\end{frame}

\watermarkoff%-----------------------------------------------------------------%

\begin{frame}[fragile]{Example}

<<>>=
dDat <- readRDS("../data/diabetes.rds")

## Focal Effect:
out0 <- lm(bp ~ bmi, data = dDat)
partSummary(out0, -c(1, 2))
@

\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}[fragile]{Example}

<<>>=
## Additive Model:
out1 <- lm(bp ~ bmi + ldl, data = dDat)
partSummary(out1, -c(1, 2))
@

\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}[fragile]{Example}

<<>>=
## Moderated Model:
out2 <- lm(bp ~ bmi * ldl, data = dDat)
partSummary(out2, -c(1, 2))
@

\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}{Visualizing the Interaction}

  \begin{columns}
    \begin{column}{0.5\textwidth}
      We can get a better idea of the patterns of moderation by plotting the
      focal effect at conditional values of the moderator.
    \end{column}

    \begin{column}{0.5\textwidth}

<<echo = FALSE>>=
m1 <- mean(dDat$ldl)
s1 <- sd(dDat$ldl)

dDat$ldlLo  <- dDat$ldl - (m1 - s1)
dDat$ldlMid <- dDat$ldl - m1
dDat$ldlHi  <- dDat$ldl - (m1 + s1)

outLo  <- lm(bp ~ bmi*ldlLo, data = dDat)
outMid <- lm(bp ~ bmi*ldlMid, data = dDat)
outHi  <- lm(bp ~ bmi*ldlHi, data = dDat)

b0Lo <- coef(outLo)[1]
b1Lo <- coef(outLo)["bmi"]

b0Mid <- coef(outMid)[1]
b1Mid <- coef(outMid)["bmi"]

b0Hi <- coef(outHi)[1]
b1Hi <- coef(outHi)["bmi"]

x    <- seq(min(dDat$bmi), max(dDat$bmi), 0.1)
dat1 <- data.frame(x    = x,
                   yLo  = b0Lo + b1Lo * x,
                   yMid = b0Mid + b1Mid * x,
                   yHi  = b0Hi + b1Hi * x)

p1 <- ggplot(data = dDat, aes(x = bmi, y = bp)) +
  theme_classic() +
  theme(text = element_text(family = "Courier", size = 16))
p2 <- p1 + geom_point(colour = "gray") +
  geom_line(mapping   = aes(x = x, y = yLo, colour = "Mean LDL - 1 SD"),
            data      = dat1,
            linewidth = 1.5) +
geom_line(mapping   = aes(x = x, y = yMid, colour = "Mean LDL"),
          data      = dat1,
          linewidth = 1.5) +
geom_line(mapping   = aes(x = x, y = yHi, colour = "Mean LDL + 1 SD"),
          data      = dat1,
          linewidth = 1.5) +
xlab("BMI") +
ylab("BP")

p2 + scale_colour_manual(name = "", values = c("Mean LDL" = "black",
                                               "Mean LDL - 1 SD" = "red",
                                               "Mean LDL + 1 SD" = "blue")
) +
theme(legend.justification = c(1, 0), legend.position = c(0.975, 0.025))
@

    \end{column}
  \end{columns}

\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}[fragile]{Example}

  Of course, we can fit the same model in \pkg{lavaan}.

<<>>=
library(lavaan)

## Specify the model:
mod <- 'bp ~ 1 + bmi + ldl + bmi:ldl'

## Estimate the model:
lavOut <- sem(mod, data = dDat)
@

\end{frame}

%%%--------------------------------------------------------------------------%%%

\begin{frame}[fragile]{Example}

<<>>=
partSummary(lavOut, 7:9)
@

\end{frame}

\watermarkon %%%-------------------------------------------------------------%%%

\subsection{Post Hoc Analysis}

%%%--------------------------------------------------------------------------%%%

\begin{frame}{Probing the Interaction}

  A significant estimate of $\beta_3$ tells us that the effect of $X$ on $Y$ 
  depends on the level of $Z$, but not much more.
  \vb
  \begin{itemize}
    \item The plot above gives a descriptive illustration of the pattern, but does 
      not support statistical inference.
      \vc
      \begin{itemize}
        \item The three conditional effects we plotted look different, but we cannot 
          say much about how they differ with only the plot and $\hat{\beta}_3$.
      \end{itemize}
      \vb
    \item This is the purpose of \emph{probing} the interaction.
      \vc
      \begin{itemize}
        \item Try to isolate areas of $Z$'s distribution in which $X \rightarrow Y$
          effect is significant and areas where it is not.
      \end{itemize}
  \end{itemize}

\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}{Probing the Interaction}

  The most popular method of probing interactions is to do a so-called 
  \emph{simple slopes} analysis.
  \vc
  \begin{itemize}
    \item Pick-a-point approach
      \vc
    \item Spotlight analysis
  \end{itemize}
  \vb
  In simple slopes analysis, we test if the slopes of the conditional effects 
  plotted above are significantly different from zero.
  \vc
  \begin{itemize}
    \item To do so, we test the significance of \emph{simple slopes}.
  \end{itemize}

\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}{Simple Slopes}

  Recall the derivation of our moderated equation:
  \begin{align*}
    Y = \beta_0 + \beta_1X + \beta_2Z + \beta_3XZ + \varepsilon
  \end{align*}
  We can reverse the process by factoring out $X$ and reordering terms:
  \begin{align*}
    Y = \beta_0 + (\beta_1 + \beta_3Z)X + \beta_2Z + \varepsilon
  \end{align*}
  Where $f(Z) = \beta_1 + \beta_3Z$ is the linear function that shows how the 
  relationship between $X$ and $Y$ changes as a function of $Z$.\\
  \vc
  \begin{center}
    \ovalbox{$f(Z)$ is the \emph{simple slope}.}
  \end{center}
  \begin{itemize}
    \item By plugging different values of $Z$ into $f(Z)$, we get the value of the 
      conditional effect of $X$ on $Y$ at the chosen level of $Z$.
  \end{itemize}

\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}{Significance Testing of Simple Slopes}

  The values of $Z$ used to define the simple slopes are arbitrary.
  \vc
  \begin{itemize}
    \item The most common choice is: $\left\{ (\bar{Z} - SD_Z), \bar{Z},
      (\bar{Z} + SD_Z) \right\}$
      \vc
    \item You could also use interesting percentiles of $Z$'s distribution.
  \end{itemize}
  \vb
  The standard error of a simple slope is given by:
  \begin{align*}
    SE_{f(Z)} = \sqrt{SE_{\beta_1}^2 + 2Z \times cov(\beta_1, \beta_3) + 
    Z^2 SE_{\beta_3}^2}
  \end{align*}
  So, you can test the significance of a simple slope by constructing a 
  t-statistic or confidence interval using $\hat{f}(Z)$ and $SE_{f(Z)}$:
  \begin{align*}
    t = \frac{\hat{f}(Z)}{SE_{f(Z)}},~~
    CI = \hat{f}(Z) \pm t_{crit} \times SE_{f(Z)}
  \end{align*}

\end{frame}

\watermarkoff %%%------------------------------------------------------------%%%

\begin{frame}[fragile]{Example}

  We can use \pkg{semTools} routines to probe interaction in \pkg{lavaan} models.
  \vc
  \begin{itemize}
    \item \src{probe2WayMC()}: simple slopes/intercepts analysis
      \vc
    \item \src{plotProbe()}: simple slopes plots
  \end{itemize}

<<>>=
library(semTools)

## Estimate and test simple slopes and simple intercepts:
ssOut <- probe2WayMC(lavOut, 
                     nameX = c("bmi", "ldl", "bmi:ldl"),
                     nameY = "bp",
                     modVar = "ldl",
                     valProbe = quantile(dDat$ldl, c(0.25, 0.50, 0.75))
)
@ 

\end{frame}

%%%--------------------------------------------------------------------------%%%

\begin{frame}[fragile]{Example}

<<>>=
## View the results:
ssOut
@ 

\end{frame}

\watermarkon %%%-------------------------------------------------------------%%%

\begin{frame}[fragile]{Example}

<<out.width = "55%">>=
## Plot the simple slopes:
plotProbe(ssOut, xlim = range(dDat$bmi), xlab = "BMI", ylab = "BP")
@ 

\end{frame}

%%%--------------------------------------------------------------------------%%%

\begin{frame}[allowframebreaks]{References}

  \bibliographystyle{apacite}
  \bibliography{../../../bibtex/tcsm.bib}

\end{frame}

%%%--------------------------------------------------------------------------%%%

\end{document}
